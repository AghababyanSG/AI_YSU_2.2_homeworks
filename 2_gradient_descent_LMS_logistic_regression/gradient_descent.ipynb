{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1PWbGOvZZg4shD-Y1m0ULHfIeHrzV3VL7","authorship_tag":"ABX9TyPK8LjJYqACBu/62ktHMgSC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"FZTJMt-pc7hc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's define a function to calculate the gradients for a given set of data. The gradients are the partial derivatives of the loss (cost) function with respect to each of the parameters. So if we have \n","$$ L(X, Y, W) = \\frac{1}{m + 1} \\big\\|XW - Y\\big\\|^2_2 = \\frac{1}{m + 1}\\sum_{i=0}^{m}(x^i W - y^i)^2$$ in this case we have $$grad(W) = \\frac{2}{m + 1}X^T(XW-Y)$$"],"metadata":{"id":"5fR9OSTbdLA4"}},{"cell_type":"code","source":["import numpy as np\n","\n","def compute_gradients(X, Y, W):\n","    '''\n","    X: input data\n","    Y: output data\n","    W: parameters (weights)\n","    return the gradient of parameters W\n","    '''\n","    m = len(Y)\n","    predictions = np.dot(X, W)\n","    errors = predictions - Y\n","    gradients = (2/m) * np.dot(X.T, errors)\n","    return gradients\n","\n","def lin_reg_l2(x, y, w):\n","    '''\n","    x: input data\n","    y: target data\n","    w: given parameters\n","    '''\n","    y_pred = np.dot(x,w)\n","    # mean square error\n","    return np.sqrt(np.mean((y_pred - y)**2))"],"metadata":{"id":"b8evJxg95Czj","executionInfo":{"status":"ok","timestamp":1676584566453,"user_tz":-240,"elapsed":7,"user":{"displayName":"Ստեփան Սարգսյան","userId":"17563089324629164814"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["Next, we define a gradient descent function, i.e. apply the gradients and update the parameter values iteratively"],"metadata":{"id":"ROKKGUuQMOGI"}},{"cell_type":"code","source":["def gradient_descent(X, y, learn_rate, num_iters, theta=None):\n","    '''\n","    X: input data\n","    y: output data\n","    learn_rate: the learning rate for gradient descent \n","    num_iterations\n","    theta: initial value for the given parameters \n","    '''\n","    m = len(y)\n","    if not theta:\n","      theta = np.zeros((X.shape[1], 1))\n","    for i in range(num_iters):\n","        gradients = compute_gradients(X, y, theta)\n","        theta = theta - learn_rate * gradients\n","        loss = lin_reg_l2(X, y, theta)\n","        print(f'{i}) loss: {loss}')\n","    return theta"],"metadata":{"id":"SxOdGy7XMJgc","executionInfo":{"status":"ok","timestamp":1676584571462,"user_tz":-240,"elapsed":5,"user":{"displayName":"Ստեփան Սարգսյան","userId":"17563089324629164814"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["To show that the gradient descent converges nearly to the same parameters as the solution of the normal equation for linear regression, we can generate some random data and compare the results of gradient descent and the normal equation:"],"metadata":{"id":"OGSR-8HJljY5"}},{"cell_type":"code","source":["# Generate random data\n","np.random.seed(0)\n","X = np.random.rand(100, 2)\n","y = 2 + np.dot(X, np.array([[3,4]]).T) + 0.1*np.random.rand(100, 1)\n","\n","# Add intercept column to X (for bias)\n","X = np.hstack((np.ones((X.shape[0], 1)), X))\n","\n","# Calculate parameters using normal equation\n","theta_normal = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n","\n","# Calculate parameters using gradient descent\n","theta_gd = gradient_descent(X, y, 0.1, 400)\n","\n","print(\"Parameters using normal equation: \", theta_normal.T)\n","print(\"Parameters using gradient descent: \", theta_gd.T)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P0agl1CLhl-g","executionInfo":{"status":"ok","timestamp":1676585064923,"user_tz":-240,"elapsed":403,"user":{"displayName":"Ստեփան Սարգսյան","userId":"17563089324629164814"}},"outputId":"c8c0da20-3a65-425f-c629-47e601b9715a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["0) loss: 3.9923695504740464\n","1) loss: 2.8107099480787543\n","2) loss: 2.0042900376700956\n","3) loss: 1.4626759300519554\n","4) loss: 1.1086673282532158\n","5) loss: 0.8864096485093699\n","6) loss: 0.7534131286648105\n","7) loss: 0.6770122454851052\n","8) loss: 0.6337548858181595\n","9) loss: 0.608609267768981\n","10) loss: 0.5929054970487517\n","11) loss: 0.5819996739708985\n","12) loss: 0.5735117725332467\n","13) loss: 0.5662508557444872\n","14) loss: 0.5596281604378663\n","15) loss: 0.5533542036243779\n","16) loss: 0.5472868472710332\n","17) loss: 0.5413561581757219\n","18) loss: 0.5355275074977497\n","19) loss: 0.529783512304015\n","20) loss: 0.5241152132494787\n","21) loss: 0.5185177681287797\n","22) loss: 0.5129883505983024\n","23) loss: 0.5075251250380641\n","24) loss: 0.5021267464119357\n","25) loss: 0.49679211620931324\n","26) loss: 0.49152026328743853\n","27) loss: 0.48631028563072143\n","28) loss: 0.48116132182103044\n","29) loss: 0.47607253699977325\n","30) loss: 0.47104311589954373\n","31) loss: 0.46607225932565566\n","32) loss: 0.4611591823223172\n","33) loss: 0.45630311316256345\n","34) loss: 0.4515032927420913\n","35) loss: 0.44675897417221755\n","36) loss: 0.4420694224720575\n","37) loss: 0.4374339143111721\n","38) loss: 0.4328517377788709\n","39) loss: 0.42832219216852074\n","40) loss: 0.42384458777114276\n","41) loss: 0.4194182456754739\n","42) loss: 0.4150424975730776\n","43) loss: 0.4107166855677809\n","44) loss: 0.40644016198904775\n","45) loss: 0.402212289209068\n","46) loss: 0.39803243946341493\n","47) loss: 0.39389999467517267\n","48) loss: 0.3898143462824485\n","49) loss: 0.3857748950691983\n","50) loss: 0.381781050999298\n","51) loss: 0.37783223305379837\n","52) loss: 0.3739278690713002\n","53) loss: 0.37006739559139273\n","54) loss: 0.36625025770109243\n","55) loss: 0.3624759088842299\n","56) loss: 0.35874381087372403\n","57) loss: 0.3550534335066906\n","58) loss: 0.3514042545823309\n","59) loss: 0.34779575972254456\n","60) loss: 0.3442274422352177\n","61) loss: 0.34069880298013133\n","62) loss: 0.337209350237443\n","63) loss: 0.33375859957868725\n","64) loss: 0.3303460737402518\n","65) loss: 0.32697130249927603\n","66) loss: 0.323633822551928\n","67) loss: 0.3203331773940131\n","68) loss: 0.3170689172038683\n","69) loss: 0.3138405987274975\n","70) loss: 0.31064778516590447\n","71) loss: 0.30749004606458097\n","72) loss: 0.30436695720510654\n","73) loss: 0.3012781004988189\n","74) loss: 0.2982230638825157\n","75) loss: 0.2952014412161453\n","76) loss: 0.2922128321824502\n","77) loss: 0.2892568421885215\n","78) loss: 0.2863330822692296\n","79) loss: 0.2834411689924921\n","80) loss: 0.2805807243663443\n","81) loss: 0.27775137574777387\n","82) loss: 0.27495275575328837\n","83) loss: 0.27218450217117846\n","84) loss: 0.2694462578754443\n","85) loss: 0.2667376707413514\n","86) loss: 0.26405839356258504\n","87) loss: 0.2614080839699689\n","88) loss: 0.25878640435172023\n","89) loss: 0.2561930217752068\n","90) loss: 0.25362760791018013\n","91) loss: 0.2510898389534504\n","92) loss: 0.24857939555497963\n","93) loss: 0.246095962745361\n","94) loss: 0.24363922986465752\n","95) loss: 0.2412088904925738\n","96) loss: 0.23880464237993296\n","97) loss: 0.2364261873814334\n","98) loss: 0.23407323138965858\n","99) loss: 0.231745484270317\n","100) loss: 0.22944265979868422\n","101) loss: 0.2271644755972264\n","102) loss: 0.22491065307437835\n","103) loss: 0.22268091736445594\n","104) loss: 0.22047499726867692\n","105) loss: 0.2182926251972706\n","106) loss: 0.2161335371126529\n","107) loss: 0.21399747247364534\n","108) loss: 0.21188417418071706\n","109) loss: 0.2097933885222301\n","110) loss: 0.20772486512166685\n","111) loss: 0.20567835688581937\n","112) loss: 0.20365361995392187\n","113) loss: 0.2016504136477077\n","114) loss: 0.19966850042237164\n","115) loss: 0.19770764581841813\n","116) loss: 0.19576761841437956\n","117) loss: 0.1938481897803857\n","118) loss: 0.19194913443256764\n","119) loss: 0.1900702297882783\n","120) loss: 0.18821125612211423\n","121) loss: 0.18637199652272218\n","122) loss: 0.18455223685037364\n","123) loss: 0.18275176569529317\n","124) loss: 0.18097037433672375\n","125) loss: 0.17920785670271552\n","126) loss: 0.17746400933062234\n","127) loss: 0.17573863132829232\n","128) loss: 0.17403152433593774\n","129) loss: 0.1723424924886708\n","130) loss: 0.17067134237969234\n","131) loss: 0.16901788302411844\n","132) loss: 0.16738192582343456\n","133) loss: 0.16576328453056147\n","134) loss: 0.16416177521552305\n","135) loss: 0.1625772162317022\n","136) loss: 0.16100942818267397\n","137) loss: 0.15945823388960226\n","138) loss: 0.15792345835919147\n","139) loss: 0.15640492875217915\n","140) loss: 0.15490247435235982\n","141) loss: 0.15341592653612918\n","142) loss: 0.1519451187425377\n","143) loss: 0.15048988644384326\n","144) loss: 0.1490500671165527\n","145) loss: 0.14762550021294293\n","146) loss: 0.14621602713304976\n","147) loss: 0.144821491197118\n","148) loss: 0.14344173761849982\n","149) loss: 0.14207661347699568\n","150) loss: 0.14072596769262583\n","151) loss: 0.1393896509998256\n","152) loss: 0.13806751592205443\n","153) loss: 0.13675941674681105\n","154) loss: 0.1354652095010456\n","155) loss: 0.1341847519269619\n","156) loss: 0.13291790345819998\n","157) loss: 0.13166452519639266\n","158) loss: 0.13042447988808747\n","159) loss: 0.1291976319020269\n","160) loss: 0.12798384720677955\n","161) loss: 0.12678299334871468\n","162) loss: 0.1255949394303129\n","163) loss: 0.12441955608880689\n","164) loss: 0.12325671547514454\n","165) loss: 0.122106291233268\n","166) loss: 0.12096815847970237\n","167) loss: 0.11984219378344738\n","168) loss: 0.11872827514616566\n","169) loss: 0.11762628198266187\n","170) loss: 0.11653609510164681\n","171) loss: 0.11545759668677916\n","172) loss: 0.11439067027798143\n","173) loss: 0.11333520075302198\n","174) loss: 0.1122910743093601\n","175) loss: 0.11125817844624608\n","176) loss: 0.1102364019470732\n","177) loss: 0.10922563486197556\n","178) loss: 0.10822576849066588\n","179) loss: 0.10723669536550956\n","180) loss: 0.10625830923483036\n","181) loss: 0.105290505046441\n","182) loss: 0.10433317893139539\n","183) loss: 0.1033862281879592\n","184) loss: 0.10244955126579101\n","185) loss: 0.10152304775033301\n","186) loss: 0.1006066183474055\n","187) loss: 0.09970016486799983\n","188) loss: 0.0988035902132692\n","189) loss: 0.09791679835970957\n","190) loss: 0.09703969434452904\n","191) loss: 0.09617218425120186\n","192) loss: 0.09531417519520195\n","193) loss: 0.09446557530991458\n","194) loss: 0.09362629373272102\n","195) loss: 0.09279624059125237\n","196) loss: 0.09197532698981263\n","197) loss: 0.0911634649959623\n","198) loss: 0.09036056762726562\n","199) loss: 0.0895665488381928\n","200) loss: 0.08878132350717836\n","201) loss: 0.0880048074238313\n","202) loss: 0.08723691727629287\n","203) loss: 0.08647757063874269\n","204) loss: 0.08572668595904734\n","205) loss: 0.08498418254655096\n","206) loss: 0.08424998056000453\n","207) loss: 0.08352400099563212\n","208) loss: 0.08280616567533151\n","209) loss: 0.082096397235007\n","210) loss: 0.08139461911303317\n","211) loss: 0.08070075553884606\n","212) loss: 0.0800147315216621\n","213) loss: 0.07933647283932083\n","214) loss: 0.07866590602725113\n","215) loss: 0.07800295836755913\n","216) loss: 0.07734755787823569\n","217) loss: 0.0766996333024828\n","218) loss: 0.0760591140981572\n","219) loss: 0.07542593042732917\n","220) loss: 0.07480001314595798\n","221) loss: 0.07418129379367872\n","222) loss: 0.07356970458370382\n","223) loss: 0.07296517839283463\n","224) loss: 0.07236764875158626\n","225) loss: 0.07177704983441954\n","226) loss: 0.07119331645008528\n","227) loss: 0.07061638403207603\n","228) loss: 0.07004618862918587\n","229) loss: 0.06948266689617968\n","230) loss: 0.06892575608456863\n","231) loss: 0.06837539403349303\n","232) loss: 0.06783151916071327\n","233) loss: 0.06729407045370635\n","234) loss: 0.06676298746087032\n","235) loss: 0.0662382102828349\n","236) loss: 0.06571967956387889\n","237) loss: 0.06520733648345571\n","238) loss: 0.06470112274782472\n","239) loss: 0.06420098058179026\n","240) loss: 0.06370685272054978\n","241) loss: 0.06321868240164798\n","242) loss: 0.06273641335704169\n","243) loss: 0.06225998980527143\n","244) loss: 0.06178935644374501\n","245) loss: 0.06132445844112926\n","246) loss: 0.06086524142985312\n","247) loss: 0.060411651498722456\n","248) loss: 0.05996363518564639\n","249) loss: 0.059521139470476464\n","250) loss: 0.05908411176796009\n","251) loss: 0.05865249992080696\n","252) loss: 0.058226252192871086\n","253) loss: 0.057805317262449606\n","254) loss: 0.05738964421569688\n","255) loss: 0.05697918254015668\n","256) loss: 0.05657388211841381\n","257) loss: 0.056173693221863066\n","258) loss: 0.05577856650459966\n","259) loss: 0.055388452997430414\n","260) loss: 0.055003304102005775\n","261) loss: 0.054623071585076004\n","262) loss: 0.05424770757286942\n","263) loss: 0.05387716454559459\n","264) loss: 0.05351139533206888\n","265) loss: 0.05315035310447105\n","266) loss: 0.052793991373220515\n","267) loss: 0.052442263981984254\n","268) loss: 0.05209512510280949\n","269) loss: 0.051752529231386096\n","270) loss: 0.05141443118243612\n","271) loss: 0.05108078608523241\n","272) loss: 0.05075154937924685\n","273) loss: 0.05042667680992731\n","274) loss: 0.05010612442460494\n","275) loss: 0.04978984856853044\n","276) loss: 0.049477805881039984\n","277) loss: 0.04916995329185219\n","278) loss: 0.04886624801749314\n","279) loss: 0.04856664755785129\n","280) loss: 0.04827110969286124\n","281) loss: 0.047979592479316935\n","282) loss: 0.04769205424781112\n","283) loss: 0.047408453599804314\n","284) loss: 0.047128749404819106\n","285) loss: 0.0468529007977606\n","286) loss: 0.04658086717636203\n","287) loss: 0.04631260819875441\n","288) loss: 0.04604808378115811\n","289) loss: 0.04578725409569676\n","290) loss: 0.045530079568330724\n","291) loss: 0.045276520876909504\n","292) loss: 0.04502653894934001\n","293) loss: 0.044780094961872144\n","294) loss: 0.044537150337495335\n","295) loss: 0.0442976667444488\n","296) loss: 0.044061606094839396\n","297) loss: 0.04382893054336802\n","298) loss: 0.04359960248616015\n","299) loss: 0.04337358455969994\n","300) loss: 0.04315083963986368\n","301) loss: 0.042931330841051336\n","302) loss: 0.04271502151541312\n","303) loss: 0.04250187525216888\n","304) loss: 0.042291855877015234\n","305) loss: 0.0420849274516217\n","306) loss: 0.04188105427320856\n","307) loss: 0.04168020087420624\n","308) loss: 0.041482332021992065\n","309) loss: 0.04128741271870122\n","310) loss: 0.041095408201108616\n","311) loss: 0.04090628394057873\n","312) loss: 0.04072000564308072\n","313) loss: 0.04053653924926279\n","314) loss: 0.040355850934586594\n","315) loss: 0.04017790710951387\n","316) loss: 0.04000267441974568\n","317) loss: 0.039830119746507874\n","318) loss: 0.039660210206880916\n","319) loss: 0.03949291315417033\n","320) loss: 0.03932819617831374\n","321) loss: 0.03916602710632141\n","322) loss: 0.03900637400274761\n","323) loss: 0.03884920517018729\n","324) loss: 0.03869448914979752\n","325) loss: 0.038542194721836905\n","326) loss: 0.03839229090622336\n","327) loss: 0.03824474696310284\n","328) loss: 0.03809953239343107\n","329) loss: 0.03795661693955829\n","330) loss: 0.03781597058582057\n","331) loss: 0.03767756355912966\n","332) loss: 0.037541366329561204\n","333) loss: 0.037407349610937474\n","334) loss: 0.03727548436140079\n","335) loss: 0.03714574178397667\n","336) loss: 0.03701809332712233\n","337) loss: 0.0368925106852596\n","338) loss: 0.03676896579928739\n","339) loss: 0.03664743085707358\n","340) loss: 0.03652787829392274\n","341) loss: 0.036410280793017065\n","342) loss: 0.0362946112858303\n","343) loss: 0.03618084295250948\n","344) loss: 0.03606894922222584\n","345) loss: 0.03595890377349063\n","346) loss: 0.035850680534434645\n","347) loss: 0.03574425368305114\n","348) loss: 0.03563959764739899\n","349) loss: 0.035536687105764186\n","350) loss: 0.035435496986781403\n","351) loss: 0.03533600246951086\n","352) loss: 0.03523817898347083\n","353) loss: 0.0351420022086258\n","354) loss: 0.035047448075327114\n","355) loss: 0.034954492764207874\n","356) loss: 0.03486311270602851\n","357) loss: 0.03477328458147527\n","358) loss: 0.03468498532090896\n","359) loss: 0.03459819210406468\n","360) loss: 0.03451288235970161\n","361) loss: 0.03442903376520315\n","362) loss: 0.03434662424612681\n","363) loss: 0.03426563197570436\n","364) loss: 0.03418603537429071\n","365) loss: 0.03410781310876453\n","366) loss: 0.03403094409187792\n","367) loss: 0.033955407481556246\n","368) loss: 0.033881182680150154\n","369) loss: 0.03380824933363724\n","370) loss: 0.03373658733077659\n","371) loss: 0.03366617680221466\n","372) loss: 0.03359699811954491\n","373) loss: 0.033529031894319834\n","374) loss: 0.03346225897701792\n","375) loss: 0.033396660455965524\n","376) loss: 0.03333221765621444\n","377) loss: 0.03326891213837606\n","378) loss: 0.03320672569741278\n","379) loss: 0.033145640361388985\n","380) loss: 0.033085638390179854\n","381) loss: 0.03302670227414278\n","382) loss: 0.0329688147327479\n","383) loss: 0.03291195871317374\n","384) loss: 0.032856117388864674\n","385) loss: 0.03280127415805423\n","386) loss: 0.03274741264225418\n","387) loss: 0.03269451668471112\n","388) loss: 0.03264257034883033\n","389) loss: 0.032591557916570785\n","390) loss: 0.032541463886809716\n","391) loss: 0.03249227297367922\n","392) loss: 0.032443970104877394\n","393) loss: 0.032396540419951464\n","394) loss: 0.03234996926855892\n","395) loss: 0.03230424220870406\n","396) loss: 0.03225934500495286\n","397) loss: 0.032215263626627584\n","398) loss: 0.03217198424598154\n","399) loss: 0.03212949323635501\n","Parameters using normal equation:  [[2.04538199 2.99820939 4.01335274]]\n","Parameters using gradient descent:  [[2.0769778  2.96733351 3.98423538]]\n"]}]},{"cell_type":"markdown","source":["As we can see, the parameters calculated using gradient descent are close to the parameters calculated using the normal equation (if iterations are chosen enough). This demonstrates that gradient descent can be a useful alternative to the normal equation for linear regression, especially for larger datasets where the normal equation may become computationally expensive."],"metadata":{"id":"v6Z5UPKWQzh1"}},{"cell_type":"markdown","source":["As a homework do the same experiment in case of Logistic regression"],"metadata":{"id":"dnZBmw4_RyO5"}}]}