{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "mount_file_id": "1PWbGOvZZg4shD-Y1m0ULHfIeHrzV3VL7",
   "authorship_tag": "ABX9TyPK8LjJYqACBu/62ktHMgSC"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Let's define a function to calculate the gradients for a given set of data. The gradients are the partial derivatives of the loss (cost) function with respect to each of the parameters. So if we have \n",
    "$$ L(X, Y, W) = \\frac{1}{m + 1} \\big\\|XW - Y\\big\\|^2_2 = \\frac{1}{m + 1}\\sum_{i=0}^{m}(x^i W - y^i)^2$$ in this case we have $$grad(W) = \\frac{2}{m + 1}X^T(XW-Y)$$"
   ],
   "metadata": {
    "id": "5fR9OSTbdLA4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_gradients(X, Y, W):\n",
    "    '''\n",
    "    X: input data\n",
    "    Y: output data\n",
    "    W: parameters (weights)\n",
    "    return the gradient of parameters W\n",
    "    '''\n",
    "    m = len(Y)\n",
    "    predictions = np.dot(X, W)\n",
    "    errors = predictions - Y\n",
    "    gradients = (2 / m) * np.dot(X.T, errors)\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def lin_reg_l2(x, y, w):\n",
    "    '''\n",
    "    x: input data\n",
    "    y: target data\n",
    "    w: given parameters\n",
    "    '''\n",
    "    y_pred = np.dot(x, w)\n",
    "    # mean square error\n",
    "    return np.sqrt(np.mean((y_pred - y) ** 2))"
   ],
   "metadata": {
    "id": "b8evJxg95Czj",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676584566453,
     "user_tz": -240,
     "elapsed": 7,
     "user": {
      "displayName": "Ստեփան Սարգսյան",
      "userId": "17563089324629164814"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-04-03T23:09:56.311681Z",
     "start_time": "2024-04-03T23:09:56.306731Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we define a gradient descent function, i.e. apply the gradients and update the parameter values iteratively"
   ],
   "metadata": {
    "id": "ROKKGUuQMOGI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def gradient_descent(X, y, learn_rate, num_iters, theta=None):\n",
    "    '''\n",
    "    X: input data\n",
    "    y: output data\n",
    "    learn_rate: the learning rate for gradient descent \n",
    "    num_iterations\n",
    "    theta: initial value for the given parameters \n",
    "    '''\n",
    "    m = len(y)\n",
    "    if not theta:\n",
    "        theta = np.zeros((X.shape[1], 1))\n",
    "    for i in range(num_iters):\n",
    "        gradients = compute_gradients(X, y, theta)\n",
    "        theta = theta - learn_rate * gradients\n",
    "        loss = lin_reg_l2(X, y, theta)\n",
    "        print(f'{i}) loss: {loss}')\n",
    "    return theta"
   ],
   "metadata": {
    "id": "SxOdGy7XMJgc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676584571462,
     "user_tz": -240,
     "elapsed": 5,
     "user": {
      "displayName": "Ստեփան Սարգսյան",
      "userId": "17563089324629164814"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-04-03T23:09:56.332081Z",
     "start_time": "2024-04-03T23:09:56.327558Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "To show that the gradient descent converges nearly to the same parameters as the solution of the normal equation for linear regression, we can generate some random data and compare the results of gradient descent and the normal equation:"
   ],
   "metadata": {
    "id": "OGSR-8HJljY5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Generate random data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 2)\n",
    "y = 2 + np.dot(X, np.array([[3, 4]]).T) + 0.1 * np.random.rand(100, 1)\n",
    "\n",
    "# Add intercept column to X (for bias)\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "# Calculate parameters using normal equation\n",
    "theta_normal = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n",
    "\n",
    "# Calculate parameters using gradient descent\n",
    "theta_gd = gradient_descent(X, y, 0.1, 400)\n",
    "\n",
    "print(\"Parameters using normal equation: \", theta_normal.T)\n",
    "print(\"Parameters using gradient descent: \", theta_gd.T)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0agl1CLhl-g",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676585064923,
     "user_tz": -240,
     "elapsed": 403,
     "user": {
      "displayName": "Ստեփան Սարգսյան",
      "userId": "17563089324629164814"
     }
    },
    "outputId": "c8c0da20-3a65-425f-c629-47e601b9715a",
    "ExecuteTime": {
     "end_time": "2024-04-03T23:09:56.359122Z",
     "start_time": "2024-04-03T23:09:56.347550Z"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) loss: 3.9923695504740464\n",
      "1) loss: 2.8107099480787543\n",
      "2) loss: 2.0042900376700956\n",
      "3) loss: 1.4626759300519554\n",
      "4) loss: 1.1086673282532158\n",
      "5) loss: 0.8864096485093699\n",
      "6) loss: 0.7534131286648105\n",
      "7) loss: 0.6770122454851051\n",
      "8) loss: 0.6337548858181596\n",
      "9) loss: 0.608609267768981\n",
      "10) loss: 0.5929054970487517\n",
      "11) loss: 0.5819996739708985\n",
      "12) loss: 0.5735117725332467\n",
      "13) loss: 0.5662508557444872\n",
      "14) loss: 0.5596281604378662\n",
      "15) loss: 0.5533542036243779\n",
      "16) loss: 0.5472868472710333\n",
      "17) loss: 0.5413561581757218\n",
      "18) loss: 0.5355275074977497\n",
      "19) loss: 0.5297835123040149\n",
      "20) loss: 0.5241152132494787\n",
      "21) loss: 0.5185177681287797\n",
      "22) loss: 0.5129883505983024\n",
      "23) loss: 0.5075251250380641\n",
      "24) loss: 0.5021267464119357\n",
      "25) loss: 0.49679211620931313\n",
      "26) loss: 0.49152026328743853\n",
      "27) loss: 0.48631028563072143\n",
      "28) loss: 0.4811613218210305\n",
      "29) loss: 0.47607253699977337\n",
      "30) loss: 0.47104311589954373\n",
      "31) loss: 0.46607225932565566\n",
      "32) loss: 0.4611591823223173\n",
      "33) loss: 0.4563031131625634\n",
      "34) loss: 0.4515032927420914\n",
      "35) loss: 0.4467589741722176\n",
      "36) loss: 0.4420694224720575\n",
      "37) loss: 0.43743391431117207\n",
      "38) loss: 0.4328517377788709\n",
      "39) loss: 0.42832219216852074\n",
      "40) loss: 0.42384458777114276\n",
      "41) loss: 0.4194182456754738\n",
      "42) loss: 0.4150424975730777\n",
      "43) loss: 0.41071668556778085\n",
      "44) loss: 0.4064401619890478\n",
      "45) loss: 0.4022122892090681\n",
      "46) loss: 0.3980324394634149\n",
      "47) loss: 0.3938999946751727\n",
      "48) loss: 0.3898143462824485\n",
      "49) loss: 0.3857748950691983\n",
      "50) loss: 0.38178105099929793\n",
      "51) loss: 0.3778322330537983\n",
      "52) loss: 0.37392786907130027\n",
      "53) loss: 0.3700673955913927\n",
      "54) loss: 0.36625025770109243\n",
      "55) loss: 0.36247590888422987\n",
      "56) loss: 0.3587438108737239\n",
      "57) loss: 0.35505343350669066\n",
      "58) loss: 0.3514042545823308\n",
      "59) loss: 0.3477957597225446\n",
      "60) loss: 0.3442274422352176\n",
      "61) loss: 0.34069880298013144\n",
      "62) loss: 0.3372093502374429\n",
      "63) loss: 0.3337585995786873\n",
      "64) loss: 0.3303460737402519\n",
      "65) loss: 0.326971302499276\n",
      "66) loss: 0.323633822551928\n",
      "67) loss: 0.32033317739401307\n",
      "68) loss: 0.3170689172038683\n",
      "69) loss: 0.3138405987274975\n",
      "70) loss: 0.3106477851659044\n",
      "71) loss: 0.307490046064581\n",
      "72) loss: 0.30436695720510665\n",
      "73) loss: 0.3012781004988189\n",
      "74) loss: 0.29822306388251574\n",
      "75) loss: 0.2952014412161453\n",
      "76) loss: 0.29221283218245026\n",
      "77) loss: 0.28925684218852155\n",
      "78) loss: 0.28633308226922954\n",
      "79) loss: 0.2834411689924921\n",
      "80) loss: 0.2805807243663443\n",
      "81) loss: 0.2777513757477739\n",
      "82) loss: 0.2749527557532883\n",
      "83) loss: 0.2721845021711785\n",
      "84) loss: 0.26944625787544435\n",
      "85) loss: 0.2667376707413515\n",
      "86) loss: 0.26405839356258504\n",
      "87) loss: 0.2614080839699689\n",
      "88) loss: 0.2587864043517201\n",
      "89) loss: 0.25619302177520686\n",
      "90) loss: 0.25362760791018013\n",
      "91) loss: 0.25108983895345044\n",
      "92) loss: 0.2485793955549797\n",
      "93) loss: 0.24609596274536094\n",
      "94) loss: 0.24363922986465752\n",
      "95) loss: 0.24120889049257385\n",
      "96) loss: 0.23880464237993296\n",
      "97) loss: 0.23642618738143334\n",
      "98) loss: 0.23407323138965855\n",
      "99) loss: 0.23174548427031702\n",
      "100) loss: 0.2294426597986843\n",
      "101) loss: 0.22716447559722638\n",
      "102) loss: 0.22491065307437838\n",
      "103) loss: 0.22268091736445592\n",
      "104) loss: 0.22047499726867695\n",
      "105) loss: 0.2182926251972706\n",
      "106) loss: 0.21613353711265296\n",
      "107) loss: 0.2139974724736454\n",
      "108) loss: 0.21188417418071703\n",
      "109) loss: 0.20979338852223017\n",
      "110) loss: 0.20772486512166696\n",
      "111) loss: 0.20567835688581934\n",
      "112) loss: 0.20365361995392184\n",
      "113) loss: 0.2016504136477078\n",
      "114) loss: 0.1996685004223717\n",
      "115) loss: 0.19770764581841818\n",
      "116) loss: 0.19576761841437956\n",
      "117) loss: 0.1938481897803858\n",
      "118) loss: 0.1919491344325677\n",
      "119) loss: 0.19007022978827834\n",
      "120) loss: 0.18821125612211426\n",
      "121) loss: 0.18637199652272213\n",
      "122) loss: 0.18455223685037364\n",
      "123) loss: 0.18275176569529314\n",
      "124) loss: 0.18097037433672375\n",
      "125) loss: 0.17920785670271552\n",
      "126) loss: 0.17746400933062237\n",
      "127) loss: 0.17573863132829234\n",
      "128) loss: 0.17403152433593774\n",
      "129) loss: 0.17234249248867078\n",
      "130) loss: 0.1706713423796923\n",
      "131) loss: 0.16901788302411852\n",
      "132) loss: 0.16738192582343467\n",
      "133) loss: 0.16576328453056158\n",
      "134) loss: 0.16416177521552316\n",
      "135) loss: 0.16257721623170235\n",
      "136) loss: 0.16100942818267408\n",
      "137) loss: 0.15945823388960234\n",
      "138) loss: 0.1579234583591916\n",
      "139) loss: 0.1564049287521793\n",
      "140) loss: 0.15490247435236\n",
      "141) loss: 0.15341592653612945\n",
      "142) loss: 0.15194511874253788\n",
      "143) loss: 0.15048988644384337\n",
      "144) loss: 0.14905006711655286\n",
      "145) loss: 0.14762550021294307\n",
      "146) loss: 0.14621602713305\n",
      "147) loss: 0.14482149119711818\n",
      "148) loss: 0.14344173761850001\n",
      "149) loss: 0.1420766134769958\n",
      "150) loss: 0.14072596769262594\n",
      "151) loss: 0.13938965099982578\n",
      "152) loss: 0.13806751592205463\n",
      "153) loss: 0.13675941674681114\n",
      "154) loss: 0.1354652095010457\n",
      "155) loss: 0.13418475192696205\n",
      "156) loss: 0.13291790345820018\n",
      "157) loss: 0.13166452519639285\n",
      "158) loss: 0.13042447988808756\n",
      "159) loss: 0.12919763190202707\n",
      "160) loss: 0.1279838472067797\n",
      "161) loss: 0.1267829933487149\n",
      "162) loss: 0.1255949394303131\n",
      "163) loss: 0.12441955608880709\n",
      "164) loss: 0.12325671547514469\n",
      "165) loss: 0.12210629123326812\n",
      "166) loss: 0.12096815847970253\n",
      "167) loss: 0.11984219378344761\n",
      "168) loss: 0.11872827514616584\n",
      "169) loss: 0.117626281982662\n",
      "170) loss: 0.11653609510164695\n",
      "171) loss: 0.1154575966867793\n",
      "172) loss: 0.1143906702779816\n",
      "173) loss: 0.11333520075302222\n",
      "174) loss: 0.1122910743093602\n",
      "175) loss: 0.11125817844624618\n",
      "176) loss: 0.1102364019470734\n",
      "177) loss: 0.10922563486197576\n",
      "178) loss: 0.10822576849066598\n",
      "179) loss: 0.10723669536550971\n",
      "180) loss: 0.10625830923483055\n",
      "181) loss: 0.10529050504644111\n",
      "182) loss: 0.10433317893139563\n",
      "183) loss: 0.10338622818795931\n",
      "184) loss: 0.1024495512657911\n",
      "185) loss: 0.1015230477503332\n",
      "186) loss: 0.10060661834740561\n",
      "187) loss: 0.09970016486799989\n",
      "188) loss: 0.09880359021326937\n",
      "189) loss: 0.09791679835970965\n",
      "190) loss: 0.09703969434452919\n",
      "191) loss: 0.09617218425120201\n",
      "192) loss: 0.09531417519520209\n",
      "193) loss: 0.09446557530991474\n",
      "194) loss: 0.09362629373272109\n",
      "195) loss: 0.09279624059125255\n",
      "196) loss: 0.09197532698981277\n",
      "197) loss: 0.0911634649959625\n",
      "198) loss: 0.09036056762726574\n",
      "199) loss: 0.08956654883819283\n",
      "200) loss: 0.08878132350717859\n",
      "201) loss: 0.08800480742383149\n",
      "202) loss: 0.08723691727629304\n",
      "203) loss: 0.0864775706387428\n",
      "204) loss: 0.08572668595904755\n",
      "205) loss: 0.08498418254655112\n",
      "206) loss: 0.08424998056000463\n",
      "207) loss: 0.08352400099563233\n",
      "208) loss: 0.08280616567533161\n",
      "209) loss: 0.08209639723500721\n",
      "210) loss: 0.08139461911303343\n",
      "211) loss: 0.08070075553884629\n",
      "212) loss: 0.08001473152166226\n",
      "213) loss: 0.07933647283932097\n",
      "214) loss: 0.07866590602725126\n",
      "215) loss: 0.07800295836755917\n",
      "216) loss: 0.07734755787823584\n",
      "217) loss: 0.07669963330248294\n",
      "218) loss: 0.07605911409815731\n",
      "219) loss: 0.07542593042732934\n",
      "220) loss: 0.0748000131459581\n",
      "221) loss: 0.0741812937936789\n",
      "222) loss: 0.0735697045837039\n",
      "223) loss: 0.07296517839283487\n",
      "224) loss: 0.07236764875158637\n",
      "225) loss: 0.07177704983441965\n",
      "226) loss: 0.07119331645008536\n",
      "227) loss: 0.07061638403207612\n",
      "228) loss: 0.07004618862918592\n",
      "229) loss: 0.06948266689617986\n",
      "230) loss: 0.06892575608456868\n",
      "231) loss: 0.06837539403349323\n",
      "232) loss: 0.06783151916071345\n",
      "233) loss: 0.06729407045370638\n",
      "234) loss: 0.06676298746087046\n",
      "235) loss: 0.06623821028283497\n",
      "236) loss: 0.065719679563879\n",
      "237) loss: 0.06520733648345589\n",
      "238) loss: 0.06470112274782473\n",
      "239) loss: 0.06420098058179034\n",
      "240) loss: 0.06370685272054981\n",
      "241) loss: 0.06321868240164814\n",
      "242) loss: 0.06273641335704172\n",
      "243) loss: 0.062259989805271476\n",
      "244) loss: 0.06178935644374507\n",
      "245) loss: 0.06132445844112925\n",
      "246) loss: 0.0608652414298532\n",
      "247) loss: 0.06041165149872246\n",
      "248) loss: 0.05996363518564633\n",
      "249) loss: 0.05952113947047657\n",
      "250) loss: 0.059084111767960225\n",
      "251) loss: 0.05865249992080701\n",
      "252) loss: 0.05822625219287122\n",
      "253) loss: 0.05780531726244964\n",
      "254) loss: 0.05738964421569684\n",
      "255) loss: 0.056979182540156735\n",
      "256) loss: 0.05657388211841379\n",
      "257) loss: 0.05617369322186308\n",
      "258) loss: 0.05577856650459967\n",
      "259) loss: 0.05538845299743036\n",
      "260) loss: 0.05500330410200575\n",
      "261) loss: 0.05462307158507612\n",
      "262) loss: 0.054247707572869376\n",
      "263) loss: 0.05387716454559453\n",
      "264) loss: 0.05351139533206884\n",
      "265) loss: 0.05315035310447099\n",
      "266) loss: 0.05279399137322042\n",
      "267) loss: 0.05244226398198423\n",
      "268) loss: 0.052095125102809496\n",
      "269) loss: 0.05175252923138613\n",
      "270) loss: 0.05141443118243606\n",
      "271) loss: 0.051080786085232324\n",
      "272) loss: 0.05075154937924681\n",
      "273) loss: 0.0504266768099273\n",
      "274) loss: 0.05010612442460495\n",
      "275) loss: 0.04978984856853036\n",
      "276) loss: 0.049477805881039894\n",
      "277) loss: 0.04916995329185208\n",
      "278) loss: 0.04886624801749306\n",
      "279) loss: 0.04856664755785117\n",
      "280) loss: 0.04827110969286114\n",
      "281) loss: 0.04797959247931678\n",
      "282) loss: 0.047692054247811054\n",
      "283) loss: 0.04740845359980426\n",
      "284) loss: 0.04712874940481902\n",
      "285) loss: 0.04685290079776047\n",
      "286) loss: 0.046580867176361895\n",
      "287) loss: 0.04631260819875431\n",
      "288) loss: 0.04604808378115799\n",
      "289) loss: 0.0457872540956966\n",
      "290) loss: 0.04553007956833073\n",
      "291) loss: 0.04527652087690934\n",
      "292) loss: 0.045026538949339924\n",
      "293) loss: 0.04478009496187202\n",
      "294) loss: 0.04453715033749523\n",
      "295) loss: 0.044297666744448724\n",
      "296) loss: 0.04406160609483929\n",
      "297) loss: 0.043828930543367894\n",
      "298) loss: 0.043599602486160044\n",
      "299) loss: 0.04337358455969989\n",
      "300) loss: 0.0431508396398635\n",
      "301) loss: 0.04293133084105117\n",
      "302) loss: 0.04271502151541295\n",
      "303) loss: 0.042501875252168766\n",
      "304) loss: 0.04229185587701514\n",
      "305) loss: 0.04208492745162156\n",
      "306) loss: 0.04188105427320842\n",
      "307) loss: 0.04168020087420604\n",
      "308) loss: 0.04148233202199199\n",
      "309) loss: 0.04128741271870109\n",
      "310) loss: 0.041095408201108435\n",
      "311) loss: 0.0409062839405787\n",
      "312) loss: 0.04072000564308066\n",
      "313) loss: 0.04053653924926271\n",
      "314) loss: 0.04035585093458645\n",
      "315) loss: 0.040177907109513654\n",
      "316) loss: 0.04000267441974553\n",
      "317) loss: 0.03983011974650773\n",
      "318) loss: 0.039660210206880826\n",
      "319) loss: 0.039492913154170306\n",
      "320) loss: 0.03932819617831359\n",
      "321) loss: 0.039166027106321305\n",
      "322) loss: 0.03900637400274738\n",
      "323) loss: 0.03884920517018717\n",
      "324) loss: 0.03869448914979742\n",
      "325) loss: 0.038542194721836814\n",
      "326) loss: 0.038392290906223106\n",
      "327) loss: 0.038244746963102726\n",
      "328) loss: 0.03809953239343084\n",
      "329) loss: 0.037956616939558145\n",
      "330) loss: 0.037815970585820306\n",
      "331) loss: 0.03767756355912944\n",
      "332) loss: 0.03754136632956099\n",
      "333) loss: 0.03740734961093731\n",
      "334) loss: 0.03727548436140058\n",
      "335) loss: 0.03714574178397642\n",
      "336) loss: 0.03701809332712211\n",
      "337) loss: 0.03689251068525946\n",
      "338) loss: 0.036768965799287234\n",
      "339) loss: 0.03664743085707344\n",
      "340) loss: 0.036527878293922504\n",
      "341) loss: 0.036410280793016905\n",
      "342) loss: 0.03629461128583003\n",
      "343) loss: 0.03618084295250925\n",
      "344) loss: 0.036068949222225616\n",
      "345) loss: 0.035958903773490355\n",
      "346) loss: 0.03585068053443452\n",
      "347) loss: 0.035744253683050946\n",
      "348) loss: 0.035639597647398824\n",
      "349) loss: 0.03553668710576389\n",
      "350) loss: 0.03543549698678123\n",
      "351) loss: 0.03533600246951065\n",
      "352) loss: 0.03523817898347068\n",
      "353) loss: 0.03514200220862553\n",
      "354) loss: 0.03504744807532706\n",
      "355) loss: 0.03495449276420767\n",
      "356) loss: 0.03486311270602836\n",
      "357) loss: 0.03477328458147512\n",
      "358) loss: 0.034684985320908836\n",
      "359) loss: 0.03459819210406462\n",
      "360) loss: 0.034512882359701486\n",
      "361) loss: 0.03442903376520299\n",
      "362) loss: 0.03434662424612677\n",
      "363) loss: 0.03426563197570419\n",
      "364) loss: 0.034186035374290664\n",
      "365) loss: 0.034107813108764505\n",
      "366) loss: 0.03403094409187783\n",
      "367) loss: 0.03395540748155626\n",
      "368) loss: 0.03388118268015006\n",
      "369) loss: 0.03380824933363712\n",
      "370) loss: 0.03373658733077654\n",
      "371) loss: 0.0336661768022147\n",
      "372) loss: 0.03359699811954491\n",
      "373) loss: 0.03352903189431969\n",
      "374) loss: 0.03346225897701775\n",
      "375) loss: 0.0333966604559654\n",
      "376) loss: 0.03333221765621443\n",
      "377) loss: 0.033268912138376024\n",
      "378) loss: 0.0332067256974128\n",
      "379) loss: 0.03314564036138884\n",
      "380) loss: 0.03308563839017978\n",
      "381) loss: 0.03302670227414268\n",
      "382) loss: 0.03296881473274782\n",
      "383) loss: 0.03291195871317367\n",
      "384) loss: 0.03285611738886459\n",
      "385) loss: 0.03280127415805416\n",
      "386) loss: 0.03274741264225419\n",
      "387) loss: 0.03269451668471112\n",
      "388) loss: 0.03264257034883039\n",
      "389) loss: 0.032591557916570736\n",
      "390) loss: 0.032541463886809584\n",
      "391) loss: 0.032492272973679244\n",
      "392) loss: 0.03244397010487732\n",
      "393) loss: 0.03239654041995145\n",
      "394) loss: 0.03234996926855888\n",
      "395) loss: 0.03230424220870402\n",
      "396) loss: 0.0322593450049529\n",
      "397) loss: 0.03221526362662757\n",
      "398) loss: 0.03217198424598149\n",
      "399) loss: 0.03212949323635488\n",
      "Parameters using normal equation:  [[2.04538199 2.99820939 4.01335274]]\n",
      "Parameters using gradient descent:  [[2.0769778  2.96733351 3.98423538]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the parameters calculated using gradient descent are close to the parameters calculated using the normal equation (if iterations are chosen enough). This demonstrates that gradient descent can be a useful alternative to the normal equation for linear regression, especially for larger datasets where the normal equation may become computationally expensive."
   ],
   "metadata": {
    "id": "v6Z5UPKWQzh1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a homework do the same experiment in case of Logistic regression"
   ],
   "metadata": {
    "id": "dnZBmw4_RyO5"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def compute_gradients_logistic(X, y, theta):\n",
    "    '''\n",
    "    X: input data\n",
    "    y: output data\n",
    "    theta: parameters (weights)\n",
    "    return the gradient of parameters theta\n",
    "    '''\n",
    "    m = len(y)\n",
    "    predictions = sigmoid(np.dot(X, theta))\n",
    "    errors = predictions - y\n",
    "    gradients = (1 / m) * np.dot(X.T, errors)\n",
    "    return gradients"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T23:09:56.363561Z",
     "start_time": "2024-04-03T23:09:56.360228Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def logistic_loss(X, y, theta):\n",
    "    '''\n",
    "    X: input data\n",
    "    y: target data\n",
    "    theta: given parameters\n",
    "    '''\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.dot(X, theta))\n",
    "    epsilon = 1e-8  # small constant to avoid division by zero\n",
    "    loss = (-1 / m) * np.sum(y * np.log(h + epsilon) + (1 - y) * np.log(1 - h + epsilon))\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T23:09:56.367473Z",
     "start_time": "2024-04-03T23:09:56.364702Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent(X, y, learn_rate, num_iters, theta=None):\n",
    "    '''\n",
    "    X: input data\n",
    "    y: output data\n",
    "    learn_rate: the learning rate for gradient descent \n",
    "    num_iterations\n",
    "    theta: initial value for the given parameters \n",
    "    '''\n",
    "    m, n = X.shape\n",
    "    if not theta:\n",
    "        theta = np.zeros((n, 1))\n",
    "    for i in range(num_iters):\n",
    "        gradients = compute_gradients_logistic(X, y, theta)\n",
    "        theta -= learn_rate * gradients\n",
    "        loss = logistic_loss(X, y, theta)\n",
    "        print(f'{i}) loss: {loss}')\n",
    "    return theta"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T23:09:56.372369Z",
     "start_time": "2024-04-03T23:09:56.368922Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) loss: -5.745160598532947\n",
      "1) loss: -11.568984190244349\n",
      "2) loss: -17.12021239855039\n",
      "3) loss: -22.575770977009217\n",
      "4) loss: -27.999878253369058\n",
      "5) loss: -33.41375154693934\n",
      "6) loss: -38.824168066348506\n",
      "7) loss: -44.23298592324469\n",
      "8) loss: -49.63962099625799\n",
      "9) loss: -55.03966061330185\n",
      "10) loss: -60.416766637709564\n",
      "11) loss: -65.71515932991608\n",
      "12) loss: -70.7693141028946\n",
      "13) loss: -75.23260291306256\n",
      "14) loss: -78.71131900898042\n",
      "15) loss: -81.04894649980339\n",
      "16) loss: -82.40498614591733\n",
      "17) loss: -83.09905612001305\n",
      "18) loss: -83.41974379861682\n",
      "19) loss: -83.55505257398475\n",
      "20) loss: -83.60802406520511\n",
      "21) loss: -83.62777665566122\n",
      "22) loss: -83.63496656084543\n",
      "23) loss: -83.63756050753479\n",
      "24) loss: -83.63849491791231\n",
      "25) loss: -83.63883203167744\n",
      "26) loss: -83.6389539790995\n",
      "27) loss: -83.63899823116446\n",
      "28) loss: -83.63901432554003\n",
      "29) loss: -83.6390202081573\n",
      "30) loss: -83.6390223527912\n",
      "31) loss: -83.63902314354007\n",
      "32) loss: -83.63902343162827\n",
      "33) loss: -83.63902353946669\n",
      "34) loss: -83.63902357726136\n",
      "35) loss: -83.63902359038126\n",
      "36) loss: -83.6390235961652\n",
      "37) loss: -83.63902359734715\n",
      "38) loss: -83.63902359770398\n",
      "39) loss: -83.63902359770398\n",
      "40) loss: -83.63902359770398\n",
      "41) loss: -83.63902359770398\n",
      "42) loss: -83.63902359770398\n",
      "43) loss: -83.63902359770398\n",
      "44) loss: -83.63902359770398\n",
      "45) loss: -83.63902359770398\n",
      "46) loss: -83.63902359770398\n",
      "47) loss: -83.63902359770398\n",
      "48) loss: -83.63902359770398\n",
      "49) loss: -83.63902359770398\n",
      "50) loss: -83.63902359770398\n",
      "51) loss: -83.63902359770398\n",
      "52) loss: -83.63902359770398\n",
      "53) loss: -83.63902359770398\n",
      "54) loss: -83.63902359770398\n",
      "55) loss: -83.63902359770398\n",
      "56) loss: -83.63902359770398\n",
      "57) loss: -83.63902359770398\n",
      "58) loss: -83.63902359770398\n",
      "59) loss: -83.63902359770398\n",
      "60) loss: -83.63902359770398\n",
      "61) loss: -83.63902359770398\n",
      "62) loss: -83.63902359770398\n",
      "63) loss: -83.63902359770398\n",
      "64) loss: -83.63902359770398\n",
      "65) loss: -83.63902359770398\n",
      "66) loss: -83.63902359770398\n",
      "67) loss: -83.63902359770398\n",
      "68) loss: -83.63902359770398\n",
      "69) loss: -83.63902359770398\n",
      "70) loss: -83.63902359770398\n",
      "71) loss: -83.63902359770398\n",
      "72) loss: -83.63902359770398\n",
      "73) loss: -83.63902359770398\n",
      "74) loss: -83.63902359770398\n",
      "75) loss: -83.63902359770398\n",
      "76) loss: -83.63902359770398\n",
      "77) loss: -83.63902359770398\n",
      "78) loss: -83.63902359770398\n",
      "79) loss: -83.63902359770398\n",
      "80) loss: -83.63902359770398\n",
      "81) loss: -83.63902359770398\n",
      "82) loss: -83.63902359770398\n",
      "83) loss: -83.63902359770398\n",
      "84) loss: -83.63902359770398\n",
      "85) loss: -83.63902359770398\n",
      "86) loss: -83.63902359770398\n",
      "87) loss: -83.63902359770398\n",
      "88) loss: -83.63902359770398\n",
      "89) loss: -83.63902359770398\n",
      "90) loss: -83.63902359770398\n",
      "91) loss: -83.63902359770398\n",
      "92) loss: -83.63902359770398\n",
      "93) loss: -83.63902359770398\n",
      "94) loss: -83.63902359770398\n",
      "95) loss: -83.63902359770398\n",
      "96) loss: -83.63902359770398\n",
      "97) loss: -83.63902359770398\n",
      "98) loss: -83.63902359770398\n",
      "99) loss: -83.63902359770398\n",
      "100) loss: -83.63902359770398\n",
      "101) loss: -83.63902359770398\n",
      "102) loss: -83.63902359770398\n",
      "103) loss: -83.63902359770398\n",
      "104) loss: -83.63902359770398\n",
      "105) loss: -83.63902359770398\n",
      "106) loss: -83.63902359770398\n",
      "107) loss: -83.63902359770398\n",
      "108) loss: -83.63902359770398\n",
      "109) loss: -83.63902359770398\n",
      "110) loss: -83.63902359770398\n",
      "111) loss: -83.63902359770398\n",
      "112) loss: -83.63902359770398\n",
      "113) loss: -83.63902359770398\n",
      "114) loss: -83.63902359770398\n",
      "115) loss: -83.63902359770398\n",
      "116) loss: -83.63902359770398\n",
      "117) loss: -83.63902359770398\n",
      "118) loss: -83.63902359770398\n",
      "119) loss: -83.63902359770398\n",
      "120) loss: -83.63902359770398\n",
      "121) loss: -83.63902359770398\n",
      "122) loss: -83.63902359770398\n",
      "123) loss: -83.63902359770398\n",
      "124) loss: -83.63902359770398\n",
      "125) loss: -83.63902359770398\n",
      "126) loss: -83.63902359770398\n",
      "127) loss: -83.63902359770398\n",
      "128) loss: -83.63902359770398\n",
      "129) loss: -83.63902359770398\n",
      "130) loss: -83.63902359770398\n",
      "131) loss: -83.63902359770398\n",
      "132) loss: -83.63902359770398\n",
      "133) loss: -83.63902359770398\n",
      "134) loss: -83.63902359770398\n",
      "135) loss: -83.63902359770398\n",
      "136) loss: -83.63902359770398\n",
      "137) loss: -83.63902359770398\n",
      "138) loss: -83.63902359770398\n",
      "139) loss: -83.63902359770398\n",
      "140) loss: -83.63902359770398\n",
      "141) loss: -83.63902359770398\n",
      "142) loss: -83.63902359770398\n",
      "143) loss: -83.63902359770398\n",
      "144) loss: -83.63902359770398\n",
      "145) loss: -83.63902359770398\n",
      "146) loss: -83.63902359770398\n",
      "147) loss: -83.63902359770398\n",
      "148) loss: -83.63902359770398\n",
      "149) loss: -83.63902359770398\n",
      "150) loss: -83.63902359770398\n",
      "151) loss: -83.63902359770398\n",
      "152) loss: -83.63902359770398\n",
      "153) loss: -83.63902359770398\n",
      "154) loss: -83.63902359770398\n",
      "155) loss: -83.63902359770398\n",
      "156) loss: -83.63902359770398\n",
      "157) loss: -83.63902359770398\n",
      "158) loss: -83.63902359770398\n",
      "159) loss: -83.63902359770398\n",
      "160) loss: -83.63902359770398\n",
      "161) loss: -83.63902359770398\n",
      "162) loss: -83.63902359770398\n",
      "163) loss: -83.63902359770398\n",
      "164) loss: -83.63902359770398\n",
      "165) loss: -83.63902359770398\n",
      "166) loss: -83.63902359770398\n",
      "167) loss: -83.63902359770398\n",
      "168) loss: -83.63902359770398\n",
      "169) loss: -83.63902359770398\n",
      "170) loss: -83.63902359770398\n",
      "171) loss: -83.63902359770398\n",
      "172) loss: -83.63902359770398\n",
      "173) loss: -83.63902359770398\n",
      "174) loss: -83.63902359770398\n",
      "175) loss: -83.63902359770398\n",
      "176) loss: -83.63902359770398\n",
      "177) loss: -83.63902359770398\n",
      "178) loss: -83.63902359770398\n",
      "179) loss: -83.63902359770398\n",
      "180) loss: -83.63902359770398\n",
      "181) loss: -83.63902359770398\n",
      "182) loss: -83.63902359770398\n",
      "183) loss: -83.63902359770398\n",
      "184) loss: -83.63902359770398\n",
      "185) loss: -83.63902359770398\n",
      "186) loss: -83.63902359770398\n",
      "187) loss: -83.63902359770398\n",
      "188) loss: -83.63902359770398\n",
      "189) loss: -83.63902359770398\n",
      "190) loss: -83.63902359770398\n",
      "191) loss: -83.63902359770398\n",
      "192) loss: -83.63902359770398\n",
      "193) loss: -83.63902359770398\n",
      "194) loss: -83.63902359770398\n",
      "195) loss: -83.63902359770398\n",
      "196) loss: -83.63902359770398\n",
      "197) loss: -83.63902359770398\n",
      "198) loss: -83.63902359770398\n",
      "199) loss: -83.63902359770398\n",
      "200) loss: -83.63902359770398\n",
      "201) loss: -83.63902359770398\n",
      "202) loss: -83.63902359770398\n",
      "203) loss: -83.63902359770398\n",
      "204) loss: -83.63902359770398\n",
      "205) loss: -83.63902359770398\n",
      "206) loss: -83.63902359770398\n",
      "207) loss: -83.63902359770398\n",
      "208) loss: -83.63902359770398\n",
      "209) loss: -83.63902359770398\n",
      "210) loss: -83.63902359770398\n",
      "211) loss: -83.63902359770398\n",
      "212) loss: -83.63902359770398\n",
      "213) loss: -83.63902359770398\n",
      "214) loss: -83.63902359770398\n",
      "215) loss: -83.63902359770398\n",
      "216) loss: -83.63902359770398\n",
      "217) loss: -83.63902359770398\n",
      "218) loss: -83.63902359770398\n",
      "219) loss: -83.63902359770398\n",
      "220) loss: -83.63902359770398\n",
      "221) loss: -83.63902359770398\n",
      "222) loss: -83.63902359770398\n",
      "223) loss: -83.63902359770398\n",
      "224) loss: -83.63902359770398\n",
      "225) loss: -83.63902359770398\n",
      "226) loss: -83.63902359770398\n",
      "227) loss: -83.63902359770398\n",
      "228) loss: -83.63902359770398\n",
      "229) loss: -83.63902359770398\n",
      "230) loss: -83.63902359770398\n",
      "231) loss: -83.63902359770398\n",
      "232) loss: -83.63902359770398\n",
      "233) loss: -83.63902359770398\n",
      "234) loss: -83.63902359770398\n",
      "235) loss: -83.63902359770398\n",
      "236) loss: -83.63902359770398\n",
      "237) loss: -83.63902359770398\n",
      "238) loss: -83.63902359770398\n",
      "239) loss: -83.63902359770398\n",
      "240) loss: -83.63902359770398\n",
      "241) loss: -83.63902359770398\n",
      "242) loss: -83.63902359770398\n",
      "243) loss: -83.63902359770398\n",
      "244) loss: -83.63902359770398\n",
      "245) loss: -83.63902359770398\n",
      "246) loss: -83.63902359770398\n",
      "247) loss: -83.63902359770398\n",
      "248) loss: -83.63902359770398\n",
      "249) loss: -83.63902359770398\n",
      "250) loss: -83.63902359770398\n",
      "251) loss: -83.63902359770398\n",
      "252) loss: -83.63902359770398\n",
      "253) loss: -83.63902359770398\n",
      "254) loss: -83.63902359770398\n",
      "255) loss: -83.63902359770398\n",
      "256) loss: -83.63902359770398\n",
      "257) loss: -83.63902359770398\n",
      "258) loss: -83.63902359770398\n",
      "259) loss: -83.63902359770398\n",
      "260) loss: -83.63902359770398\n",
      "261) loss: -83.63902359770398\n",
      "262) loss: -83.63902359770398\n",
      "263) loss: -83.63902359770398\n",
      "264) loss: -83.63902359770398\n",
      "265) loss: -83.63902359770398\n",
      "266) loss: -83.63902359770398\n",
      "267) loss: -83.63902359770398\n",
      "268) loss: -83.63902359770398\n",
      "269) loss: -83.63902359770398\n",
      "270) loss: -83.63902359770398\n",
      "271) loss: -83.63902359770398\n",
      "272) loss: -83.63902359770398\n",
      "273) loss: -83.63902359770398\n",
      "274) loss: -83.63902359770398\n",
      "275) loss: -83.63902359770398\n",
      "276) loss: -83.63902359770398\n",
      "277) loss: -83.63902359770398\n",
      "278) loss: -83.63902359770398\n",
      "279) loss: -83.63902359770398\n",
      "280) loss: -83.63902359770398\n",
      "281) loss: -83.63902359770398\n",
      "282) loss: -83.63902359770398\n",
      "283) loss: -83.63902359770398\n",
      "284) loss: -83.63902359770398\n",
      "285) loss: -83.63902359770398\n",
      "286) loss: -83.63902359770398\n",
      "287) loss: -83.63902359770398\n",
      "288) loss: -83.63902359770398\n",
      "289) loss: -83.63902359770398\n",
      "290) loss: -83.63902359770398\n",
      "291) loss: -83.63902359770398\n",
      "292) loss: -83.63902359770398\n",
      "293) loss: -83.63902359770398\n",
      "294) loss: -83.63902359770398\n",
      "295) loss: -83.63902359770398\n",
      "296) loss: -83.63902359770398\n",
      "297) loss: -83.63902359770398\n",
      "298) loss: -83.63902359770398\n",
      "299) loss: -83.63902359770398\n",
      "300) loss: -83.63902359770398\n",
      "301) loss: -83.63902359770398\n",
      "302) loss: -83.63902359770398\n",
      "303) loss: -83.63902359770398\n",
      "304) loss: -83.63902359770398\n",
      "305) loss: -83.63902359770398\n",
      "306) loss: -83.63902359770398\n",
      "307) loss: -83.63902359770398\n",
      "308) loss: -83.63902359770398\n",
      "309) loss: -83.63902359770398\n",
      "310) loss: -83.63902359770398\n",
      "311) loss: -83.63902359770398\n",
      "312) loss: -83.63902359770398\n",
      "313) loss: -83.63902359770398\n",
      "314) loss: -83.63902359770398\n",
      "315) loss: -83.63902359770398\n",
      "316) loss: -83.63902359770398\n",
      "317) loss: -83.63902359770398\n",
      "318) loss: -83.63902359770398\n",
      "319) loss: -83.63902359770398\n",
      "320) loss: -83.63902359770398\n",
      "321) loss: -83.63902359770398\n",
      "322) loss: -83.63902359770398\n",
      "323) loss: -83.63902359770398\n",
      "324) loss: -83.63902359770398\n",
      "325) loss: -83.63902359770398\n",
      "326) loss: -83.63902359770398\n",
      "327) loss: -83.63902359770398\n",
      "328) loss: -83.63902359770398\n",
      "329) loss: -83.63902359770398\n",
      "330) loss: -83.63902359770398\n",
      "331) loss: -83.63902359770398\n",
      "332) loss: -83.63902359770398\n",
      "333) loss: -83.63902359770398\n",
      "334) loss: -83.63902359770398\n",
      "335) loss: -83.63902359770398\n",
      "336) loss: -83.63902359770398\n",
      "337) loss: -83.63902359770398\n",
      "338) loss: -83.63902359770398\n",
      "339) loss: -83.63902359770398\n",
      "340) loss: -83.63902359770398\n",
      "341) loss: -83.63902359770398\n",
      "342) loss: -83.63902359770398\n",
      "343) loss: -83.63902359770398\n",
      "344) loss: -83.63902359770398\n",
      "345) loss: -83.63902359770398\n",
      "346) loss: -83.63902359770398\n",
      "347) loss: -83.63902359770398\n",
      "348) loss: -83.63902359770398\n",
      "349) loss: -83.63902359770398\n",
      "350) loss: -83.63902359770398\n",
      "351) loss: -83.63902359770398\n",
      "352) loss: -83.63902359770398\n",
      "353) loss: -83.63902359770398\n",
      "354) loss: -83.63902359770398\n",
      "355) loss: -83.63902359770398\n",
      "356) loss: -83.63902359770398\n",
      "357) loss: -83.63902359770398\n",
      "358) loss: -83.63902359770398\n",
      "359) loss: -83.63902359770398\n",
      "360) loss: -83.63902359770398\n",
      "361) loss: -83.63902359770398\n",
      "362) loss: -83.63902359770398\n",
      "363) loss: -83.63902359770398\n",
      "364) loss: -83.63902359770398\n",
      "365) loss: -83.63902359770398\n",
      "366) loss: -83.63902359770398\n",
      "367) loss: -83.63902359770398\n",
      "368) loss: -83.63902359770398\n",
      "369) loss: -83.63902359770398\n",
      "370) loss: -83.63902359770398\n",
      "371) loss: -83.63902359770398\n",
      "372) loss: -83.63902359770398\n",
      "373) loss: -83.63902359770398\n",
      "374) loss: -83.63902359770398\n",
      "375) loss: -83.63902359770398\n",
      "376) loss: -83.63902359770398\n",
      "377) loss: -83.63902359770398\n",
      "378) loss: -83.63902359770398\n",
      "379) loss: -83.63902359770398\n",
      "380) loss: -83.63902359770398\n",
      "381) loss: -83.63902359770398\n",
      "382) loss: -83.63902359770398\n",
      "383) loss: -83.63902359770398\n",
      "384) loss: -83.63902359770398\n",
      "385) loss: -83.63902359770398\n",
      "386) loss: -83.63902359770398\n",
      "387) loss: -83.63902359770398\n",
      "388) loss: -83.63902359770398\n",
      "389) loss: -83.63902359770398\n",
      "390) loss: -83.63902359770398\n",
      "391) loss: -83.63902359770398\n",
      "392) loss: -83.63902359770398\n",
      "393) loss: -83.63902359770398\n",
      "394) loss: -83.63902359770398\n",
      "395) loss: -83.63902359770398\n",
      "396) loss: -83.63902359770398\n",
      "397) loss: -83.63902359770398\n",
      "398) loss: -83.63902359770398\n",
      "399) loss: -83.63902359770398\n",
      "Parameters using gradient descent for logistic regression:  [[181.70303785 181.70303785 102.01473236 100.90638615]]\n"
     ]
    }
   ],
   "source": [
    "# X = np.random.rand(100, 2)\n",
    "# y = 2 + np.dot(X, np.array([[3, 4]]).T) + 0.1 * np.random.rand(100, 1)\n",
    "\n",
    "# Add intercept column to X (for bias)\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "theta_logistic_gd = logistic_regression_gradient_descent(X, y, 0.1, 400)\n",
    "\n",
    "print(\"Parameters using gradient descent for logistic regression: \", theta_logistic_gd.T)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T23:09:56.386687Z",
     "start_time": "2024-04-03T23:09:56.373376Z"
    }
   },
   "execution_count": 16
  }
 ]
}
